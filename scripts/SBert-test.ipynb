{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/nfs/users/xueyou/github/models')\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/users/xueyou/github/Bert-for-Chinese-NLP\n"
     ]
    }
   ],
   "source": [
    "cd /nfs/users/xueyou/github/Bert-for-Chinese-NLP/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import siamese_bert\n",
    "from official.nlp.bert import configs as bert_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_DIR='/data/xueyou/data/bert_pretrain/converted/chinese_L-12_H-768_A-12'\n",
    "bert_config = bert_configs.BertConfig.from_json_file(BERT_DIR+'/bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_siamese, encoder = siamese_bert.siamese_model(bert_config,2,'triplet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert_siamese.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_word_ids:0' shape=(None, None) dtype=int32>,\n",
       " <tf.Tensor 'input_mask:0' shape=(None, None) dtype=int32>,\n",
       " <tf.Tensor 'input_type_ids:0' shape=(None, None) dtype=int32>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'dropout_1/cond/Identity:0' shape=(None, 768) dtype=float32>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6cb4235f98>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_siamese.load_weights('/data/xueyou/data/corpus/task_data/lcqmc/triplet/best/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert_siamese.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from official.nlp.bert import model_saving_utils, tokenization\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(BERT_DIR + '/vocab.txt', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import dataset,input_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(dataset.TripletDataset):\n",
    "    def filter_example(self, item):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = MyData(\n",
    "    tokenizer, \n",
    "    '/data/xueyou/data/corpus/task_data/lcqmc/test.json', \n",
    "    'eval', \n",
    "    './labels/binary.txt', \n",
    "    128)\n",
    "dev_data = MyData(\n",
    "    tokenizer, \n",
    "    '/data/xueyou/data/corpus/task_data/lcqmc/dev.json', \n",
    "    'eval', \n",
    "    './labels/binary.txt', \n",
    "    128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = input_pipeline.create_siamese_dataset(test_data,64,False,None)\n",
    "dev_dataset = input_pipeline.create_siamese_dataset(dev_data,64,False,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=128\n",
    "BERT_UNKOWN_TOKEN = \"[UNK]\"\n",
    "BERT_CLS_TOKEN = \"[CLS]\"\n",
    "BERT_SEP_TOKEN = \"[SEP]\"\n",
    "BERT_MASK_TOKEN = \"[MASK]\"\n",
    "def encode_single_sentence(sentence, segment=0):\n",
    "    tokens = list(tokenizer.tokenize(sentence))\n",
    "    # Account for CLS and SEP\n",
    "    tokens = tokens[0:max_seq_length-2]\n",
    "    tokens = [BERT_CLS_TOKEN] + tokens + [BERT_SEP_TOKEN]\n",
    "    mask = [1] * len(tokens)\n",
    "    return {\n",
    "      'input_word_ids': np.asarray([tokenizer.convert_tokens_to_ids(tokens)]),\n",
    "      'input_mask': np.asarray([mask]),\n",
    "      'input_type_ids': np.asarray([[segment] * len(tokens)])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 768\n",
    "ann = AnnoyIndex(f, 'euclidean') \n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = defaultdict(set)\n",
    "raw_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12500it [00:00, 67309.26it/s]\n",
      "8802it [00:00, 56396.91it/s]\n"
     ]
    }
   ],
   "source": [
    "negtives = {}\n",
    "\n",
    "for item in tqdm(open('/data/xueyou/data/corpus/task_data/lcqmc/test.json')):\n",
    "    item = json.loads(item)\n",
    "    raw_texts.append(item['text_a'])\n",
    "    raw_texts.append(item['text_b'])\n",
    "    if item['label'] == '0':\n",
    "        negtives[item['text_a']] = item['text_b']\n",
    "    texts[item['text_a']].add(item['text_b'])\n",
    "    texts[item['text_b']].add(item['text_a'])\n",
    "    \n",
    "for item in tqdm(open('/data/xueyou/data/corpus/task_data/lcqmc/dev.json')):\n",
    "    item = json.loads(item)\n",
    "    raw_texts.append(item['text_a'])\n",
    "    raw_texts.append(item['text_b'])\n",
    "    if item['label'] == '0':\n",
    "        negtives[item['text_a']] = item['text_b']\n",
    "    texts[item['text_a']].add(item['text_b'])\n",
    "    texts[item['text_b']].add(item['text_a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(texts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [00:39,  4.91it/s]\n",
      "138it [00:30,  4.56it/s]\n"
     ]
    }
   ],
   "source": [
    "cc = 0\n",
    "for x in tqdm(test_dataset):\n",
    "    (x1,x2),y = x\n",
    "    cc += len(y)\n",
    "    vals = model(x1)\n",
    "    vals2 = model(x2)\n",
    "    for v1,v2 in zip(vals.numpy(),vals2.numpy()):\n",
    "        ann.add_item(cnt,v1)\n",
    "        cnt += 1\n",
    "        ann.add_item(cnt,v2)\n",
    "        cnt += 1\n",
    "\n",
    "for x in tqdm(dev_dataset):\n",
    "    (x1,x2),y = x\n",
    "    cc += len(y)\n",
    "    vals = model(x1)\n",
    "    vals2 = model(x2)\n",
    "    for v1,v2 in zip(vals.numpy(),vals2.numpy()):\n",
    "        ann.add_item(cnt,v1)\n",
    "        cnt += 1\n",
    "        ann.add_item(cnt,v2)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.build(10) # 10 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'谁有狂三这张高清的'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "keys[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet_sims(idx,text=''):\n",
    "    if text:\n",
    "        v = model(encode_single_sentence(text))[0]\n",
    "    else:\n",
    "        v = model(encode_single_sentence(keys[idx]))[0]\n",
    "    cands = set()\n",
    "    for x,y in zip(*ann.get_nns_by_vector(v.numpy(), 10, -1,include_distances=True)):\n",
    "        w = raw_texts[x]\n",
    "        if w in cands:\n",
    "            continue\n",
    "        cands.add(w)\n",
    "        print(w,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "谁有狂三这张高清的 1.077336219168501e-05\n",
      "这张高清图，谁有 6.200240612030029\n",
      "求这张图的高清大图~ 6.374392509460449\n",
      "求这张动漫高清大图… 6.45466947555542\n",
      "求艋胛这张图片的高清大图 6.558984279632568\n",
      "求这张图的高清大图 6.5802693367004395\n",
      "这张图的高清大图谁有 6.615054130554199\n",
      "有这张图的高清大图吗 6.805208683013916\n",
      "求这张动漫图高清大图！ 6.8919854164123535\n",
      "求这个图片的电脑壁纸～要高清 6.9003424644470215\n"
     ]
    }
   ],
   "source": [
    "get_triplet_sims(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'这张高清图，谁有'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[keys[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这张高清图，谁有'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negtives.get(keys[idx],'无')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ams_siamese, _ = siamese_bert.siamese_model(bert_config,2,'ams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6d940ce6a0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ams_siamese.load_weights('/data/xueyou/data/corpus/task_data/lcqmc/ams/best/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ams_model = ams_siamese.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 768\n",
    "ann2 = AnnoyIndex(f, 'dot') \n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [00:32,  6.01it/s]\n",
      "138it [00:25,  5.45it/s]\n"
     ]
    }
   ],
   "source": [
    "cc = 0\n",
    "for x in tqdm(test_dataset):\n",
    "    (x1,x2),y = x\n",
    "    cc += len(y)\n",
    "    vals = ams_model(x1)\n",
    "    vals2 = ams_model(x2)\n",
    "    for v1,v2 in zip(vals.numpy(),vals2.numpy()):\n",
    "        ann2.add_item(cnt,v1)\n",
    "        cnt += 1\n",
    "        ann2.add_item(cnt,v2)\n",
    "        cnt += 1\n",
    "\n",
    "for x in tqdm(dev_dataset):\n",
    "    (x1,x2),y = x\n",
    "    cc += len(y)\n",
    "    vals = ams_model(x1)\n",
    "    vals2 = ams_model(x2)\n",
    "    for v1,v2 in zip(vals.numpy(),vals2.numpy()):\n",
    "        ann2.add_item(cnt,v1)\n",
    "        cnt += 1\n",
    "        ann2.add_item(cnt,v2)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann2.build(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ams_sims(idx,text=''):\n",
    "    if text:\n",
    "        v = ams_model(encode_single_sentence(text))[0]\n",
    "    else:\n",
    "        v = ams_model(encode_single_sentence(keys[idx]))[0]\n",
    "    cands = set()\n",
    "    for x,y in zip(*ann2.get_nns_by_vector(v.numpy(), 10, -1, include_distances=True)):\n",
    "        w = raw_texts[x]\n",
    "        if w in cands:\n",
    "            continue\n",
    "        cands.add(w)\n",
    "        print(w,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++\n",
      "原始query - 男的，短发能做什么发型呢？\n",
      "真实相似  - {'短发到肩，什么发型好？'}\n",
      "反例      - 短发到肩，什么发型好？\n",
      "####################\n",
      "男的，短发能做什么发型呢？ 9.351257176604122e-06\n",
      "短发到肩，什么发型好？ 4.331512928009033\n",
      "男生头发软适合烫什么发型？ 4.906976222991943\n",
      "短发烫什么发型好看 4.93504524230957\n",
      "男头发少适合什么发型 5.176742076873779\n",
      "男生长发弄什么发型好看？ 5.197969913482666\n",
      "中短发烫什么发型好看？ 5.229706764221191\n",
      "男生头发软适合留什么发型 5.305871963500977\n",
      "想把头发剪短，适合什么发型呢？ 5.351088047027588\n",
      "我这样的头发剪什么发型好看！ 5.354901313781738\n",
      "====================\n",
      "男的，短发能做什么发型呢？ 1.0\n",
      "男生长发弄什么发型好看？ 0.9980127811431885\n",
      "男生什么长发发型好看？ 0.9972010254859924\n",
      "适合做什么发型 0.9970504641532898\n",
      "求男孩不烫发好看一些的发型 0.9968783855438232\n",
      "谁知道这个发型怎么扎 0.9967594742774963\n",
      "做什么发型 0.9965689778327942\n",
      "谁知道这个发型是怎么梳的啊 0.9962197542190552\n",
      "++++++++++++++++++++\n",
      "原始query - 有什么适合女生玩的游戏\n",
      "真实相似  - {'上海有什么地方适合情侣玩的'}\n",
      "反例      - 上海有什么地方适合情侣玩的\n",
      "####################\n",
      "有什么适合女生玩的游戏 1.1066676961490884e-05\n",
      "现在什么游戏好玩啊？来个朋友推荐下。 7.59814453125\n",
      "什么游戏适合上班族玩？ 7.6697235107421875\n",
      "谁能推荐我几个好玩的小游戏？ 7.91718864440918\n",
      "推荐几个好玩的小游戏。 8.277565002441406\n",
      "推荐几个好玩的小游戏，越多越好 8.634293556213379\n",
      "什么舞蹈适合女生跳？ 9.17080020904541\n",
      "养宠物的游戏 9.251593589782715\n",
      "====================\n",
      "有什么适合女生玩的游戏 1.0\n",
      "有没有这样的游戏 0.9866812825202942\n",
      "有没有什么适合女生玩的网页游戏？ 0.9858647584915161\n",
      "有没有很黄的游戏 0.9854868650436401\n",
      "有没有什么适合上班族玩的游戏？ 0.9851201772689819\n",
      "有什么适合女生玩的网页游戏？ 0.9847934246063232\n",
      "四个人玩的游戏 0.9844945073127747\n",
      "什么游戏适合上班族玩？ 0.9835005402565002\n",
      "什么游戏好玩呢。 0.9832035899162292\n",
      "++++++++++++++++++++\n",
      "原始query - 口袋妖怪漆黑的魅影5.0手机版金手指\n",
      "真实相似  - {'手机版口袋妖怪漆黑的魅影5.0金手指'}\n",
      "反例      - 无\n",
      "####################\n",
      "口袋妖怪漆黑的魅影5.0手机版金手指 1.1864036423503421e-05\n",
      "手机版口袋妖怪漆黑的魅影5.0金手指 1.0893921852111816\n",
      "口袋妖怪漆黑的魅影怎么装金手指 3.3046183586120605\n",
      "口袋妖怪漆黑魅影金手指怎么用？ 3.6571686267852783\n",
      "魔幻手机还有第三部吗 7.887600421905518\n",
      "我的世界活塞怎么合成 8.023078918457031\n",
      "魔幻手机会出第三部吗？ 8.330820083618164\n",
      "我是你的小苹果游戏记录截屏 8.44510269165039\n",
      "====================\n",
      "口袋妖怪漆黑的魅影5.0手机版金手指 1.0\n",
      "手机版口袋妖怪漆黑的魅影5.0金手指 0.9997405409812927\n",
      "我的世界1.8怎么合成不了玻璃瓶 0.9874265789985657\n",
      "NDS口袋妖怪哪个最好玩，说说各个版本的相同处。 0.9835540056228638\n",
      "怎么下载奥拉星的黄色小人 0.983547568321228\n",
      "口袋妖怪那个版本好玩 0.9830513000488281\n",
      "时空猎人有破解版吗？ 0.9827155470848083\n",
      "剑三盒子怎么用 0.9824326038360596\n",
      "口袋妖怪哪个版本好玩 0.9821381568908691\n",
      "口袋妖怪哪个版本最好玩？ 0.9818735122680664\n",
      "++++++++++++++++++++\n",
      "原始query - 全世界一共有多少种球类竞技活动?\n",
      "真实相似  - {'全世界有多少种球类比赛'}\n",
      "反例      - 无\n",
      "####################\n",
      "全世界一共有多少种球类竞技活动? 1.1445202289905865e-05\n",
      "全世界有多少种球类比赛 4.007887840270996\n",
      "大家为什么喜欢看世界杯 8.8009614944458\n",
      "乐高星球大战怎么下 8.935287475585938\n",
      "乐高星球大战怎么用男的 9.08185863494873\n",
      "战机世界玩的人多吗 9.17149829864502\n",
      "球类里面那个球飞行速度最快？ 9.197734832763672\n",
      "所有的足球动漫都有哪些 9.256644248962402\n",
      "====================\n",
      "全世界一共有多少种球类竞技活动? 1.0\n",
      "斗地主有什么技巧？ 0.9491920471191406\n",
      "边锋港式五张不能玩？哪里可以玩呀？ 0.9409968852996826\n",
      "哪里可以看点金胜手 0.9317398071289062\n",
      "打麻将怎么赢 0.9217053651809692\n",
      "红警大战怎么玩 0.9198555946350098\n",
      "DNF测试一下自己适合什么职业 0.918042778968811\n",
      "部落冲突怎么搞的 0.9174287915229797\n",
      "++++++++++++++++++++\n",
      "原始query - 传奇世界小问题\n",
      "真实相似  - {'传奇世界的问题'}\n",
      "反例      - 无\n",
      "####################\n",
      "传奇世界小问题 9.674201464804355e-06\n",
      "奥比岛神秘世界在那里 7.2959747314453125\n",
      "我的世界怎么做活塞门 7.608443737030029\n",
      "手机版我的世界怎么玩生存 7.629197597503662\n",
      "我的世界游戏 7.670708656311035\n",
      "我的世界手机版有铁傀儡吗 7.690789699554443\n",
      "谁教我玩我的世界生存模式？ 7.74348783493042\n",
      "我的世界怎么合成 7.744981288909912\n",
      "====================\n",
      "传奇世界小问题 1.0000001192092896\n",
      "传奇世界的问题 0.9981885552406311\n",
      "我的世界谁帮我开服高悬赏 0.9934147596359253\n",
      "梦幻西游什么时候能不要点卡 0.9925723075866699\n",
      "梦幻西游刷成就点 0.9920624494552612\n",
      "神武手游盘丝怎么加点 0.992007315158844\n",
      "为什么我的魔兽世界更新不了？ 0.9919670820236206\n",
      "迷你西游都有什么新手礼包？ 0.9919492602348328\n",
      "我的世界生存模式鞍怎么用 0.9917576313018799\n",
      "我的世界怎么合成 0.9917187094688416\n"
     ]
    }
   ],
   "source": [
    "for i in random.sample(list(range(len(keys))),5):\n",
    "    print('+'*20)\n",
    "    print('原始query -',keys[i])\n",
    "    print('真实相似  -',texts[keys[i]]) \n",
    "    print('反例      -',negtives.get(keys[i],'无'))\n",
    "    print('#'*20)\n",
    "    get_triplet_sims(i)\n",
    "    print('='*20)\n",
    "    get_ams_sims(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "晚上尿多吃什么药 0.9857357740402222\n",
      "糖尿病人能吃什么 0.9809780716896057\n",
      "糖尿病人吃什么好 0.9809621572494507\n",
      "刚感冒吃什么药啊 0.9778760075569153\n",
      "肝病吃什么会好 0.9777190089225769\n",
      "感冒了吃啥最好？ 0.9771125912666321\n",
      "糖尿病人感冒吃什么好 0.9769859910011292\n"
     ]
    }
   ],
   "source": [
    "get_ams_sims(0,'你今天吃药了吗')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天早上没吃药，感觉自己萌萌哒 7.085054397583008\n",
      "晚上尿多吃什么药 7.133637428283691\n",
      "今天没吃药感觉自己萌萌哒！ 7.230893611907959\n",
      "感冒了吃啥最好？ 7.69866418838501\n",
      "被猫咬了要打针吗 7.723515510559082\n",
      "今天中午吃什么 7.772645473480225\n",
      "什么药可以让人昏迷 7.775844097137451\n",
      "隔夜蛋可以吃吗 7.888108253479004\n"
     ]
    }
   ],
   "source": [
    "get_triplet_sims(0,'你今天吃药了吗')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87022275"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(ams_model(encode_single_sentence('你今天吃药了吗'))[0],ams_model(encode_single_sentence('今天药有没有服用'))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5063596"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(model(encode_single_sentence('你今天吃药了吗'))[0]-model(encode_single_sentence('今天药有没有服用'))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "jason_pytorch_tf2.2",
   "language": "python",
   "name": "jason_pytorch_tf2.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
